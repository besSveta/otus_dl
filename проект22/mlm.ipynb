{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b18fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers import  normalizers, decoders\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from tokenizers.processors import RobertaProcessing\n",
    "path = \"osetian_corpus6.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67f16a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/OssetBERTo-small7/vocab.json',\n",
       " './models/OssetBERTo-small7/merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize a tokenizer\n",
    "'''\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"\\n\", \"‚ñÅ\"),# this is *NOT* underscore but a rare unicode char\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents()\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "#bert_normalizer = BertNormalizer(clean_text=True,handle_chinese_chars=False)\n",
    "#tokenizer._tokenizer.normalizer =  normalizers.Sequence([bert_normalizer, normalizers.NFKC()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "vocab_size = 60_000\n",
    "max_length = 512\n",
    "# Customize training\n",
    "tokenizer.train(files=path, vocab_size=vocab_size, min_frequency=2, special_tokens=[\n",
    "   \"[PAD]\",\"[UNK]\",\"[MASK]\", \"[SEP]\",\"[CLS]\"\n",
    "])\n",
    "\n",
    "tokenizer._tokenizer.post_processor = RobertaProcessing((\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")), \n",
    "                                                         (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),)\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_model(\"./models/OssetBERTo-small7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files to disk\n",
    "#tokenizer.save_model(\"./models/OssetBERTo-small3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa983675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./models/OssetBERTo-small6\"\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path, )\n",
    "tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a456b38",
   "metadata": {},
   "source": [
    "import json\n",
    "with open(\"osset_bert-vocab.json\", encoding=\"utf-8\") as p:\n",
    "    jsonstr =  json.loads(p.read())\n",
    "jsonstr    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20cc068",
   "metadata": {},
   "source": [
    "with open(\"osset_bert-merges.txt\", encoding=\"utf-8\", mode=\"r\") as p:\n",
    "    merges =  p.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7bf84e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=3, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]', '√ëƒ•√ê¬∞√ëƒ®√ê¬∞√ëƒ•', '[SEP]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from tokenizers import implementations , Tokenizer\n",
    "from tokenizers.processors import RobertaProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer.from_file(\"./models/OssetBERTo-small7/vocab.json\", \n",
    "                                            \"./models/OssetBERTo-small7/merges.txt\")\n",
    "tokenizer._tokenizer.post_processor = RobertaProcessing(\n",
    "(\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")), (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "print(\n",
    "    tokenizer.encode(\"—É–∞—Ü–∞—É\")\n",
    ")\n",
    "# Encoding(num_tokens=7, ...)\n",
    "# tokens: ['<s>', 'Mi', 'ƒ†estas', 'ƒ†Juli', 'en', '.', '</s>']\n",
    "m = tokenizer.encode(\"—É–∞—Ü–∞—É\")\n",
    "m.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.id_to_token(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e66566c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id('<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a889418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—É–∞—Ü–∞—É\n",
      "—É–∞—Ü–∞—É\n",
      "—É–∞—Ü–∞—É\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.decoders import ByteLevel\n",
    "decoder = ByteLevel()\n",
    "for token in m.tokens:\n",
    "    print(decoder.decode( '√ëƒ•√ê¬∞√ëƒ®√ê¬∞√ëƒ•'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ad60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loader\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from datasets import load_dataset\n",
    "model_path = \"./models/OssetBERTo-small4\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path, max_len=512)\n",
    "file_path  = f\"osetian_corpus4.txt\"\n",
    "dataset = load_dataset('text', data_files={'train':file_path})\n",
    "tokenized_data = dataset.map( preprocess_function, batched=True, num_proc=4, \n",
    "                             remove_columns=dataset[\"train\"].column_names, )\n",
    "block_size = 128\n",
    "dataset =  tokenized_data.map(group_texts, batched=True, batch_size=1000, num_proc=4)\n",
    "\n",
    "dataset.column_names\n",
    "\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, )\n",
    "dataset.set_format(\"torch\")\n",
    "loader = torch.utils.data.DataLoader(dataset['train'], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd9bb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60000, 57240, 60001]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('—É–∞—Ü–∞—É')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45a89f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>—É–∞—Ü–∞—É</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([60000, 57240, 60001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c6005",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39515af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import  normalizers, decoders\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaConfig\n",
    "vocab_size = 100_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f278e16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e569848209fdac70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/sveta/.cache/huggingface/datasets/text/default-e569848209fdac70/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54b5ca7c2314855ab568693e7cba9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20281bb5e1204df5a6321efc4d4440ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/sveta/.cache/huggingface/datasets/text/default-e569848209fdac70/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4452100a9b41498095f799ad0ce21255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./models/OssetBERTo-small5/tokenizer.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small5/added_tokens.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small5/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small5/tokenizer_config.json. We won't load it.\n",
      "loading file ./models/OssetBERTo-small5/vocab.json\n",
      "loading file ./models/OssetBERTo-small5/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ./models/OssetBERTo-small5/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small5\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n",
      "loading configuration file ./models/OssetBERTo-small5/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small5\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "file_path  = f\"osetian_corpus5.txt\"\n",
    "dataset = load_dataset('text', data_files={'train':file_path})\n",
    "dataset  = dataset['train']\n",
    "dataset  = dataset.train_test_split(test_size=0.15)\n",
    "\n",
    "\n",
    "model_path = \"./models/OssetBERTo-small5\"\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94155c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer( examples['text'])\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a92604",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "#dataset = dataset.flatten()\n",
    "#batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n",
    "\n",
    "tokenized_data = dataset.map( preprocess_function, batched=True, num_proc=4, \n",
    "                             remove_columns=dataset[\"train\"].column_names, )\n",
    "dataset =  tokenized_data.map(group_texts, batched=True, batch_size=1000, num_proc=4)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size= vocab_size,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_gpu_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset= dataset[\"test\"],\n",
    ")\n",
    "old_collator = trainer.data_collator\n",
    "trainer.data_collator = lambda data: dict(old_collator(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_path)\n",
    "#import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfc25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "import math\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3a4b6",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a8ba7872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models/OssetBERTo-small6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small6\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n",
      "loading configuration file ./models/OssetBERTo-small6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small6\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n",
      "loading weights file ./models/OssetBERTo-small6/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./models/OssetBERTo-small6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ./models/OssetBERTo-small6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small6\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n",
      "Didn't find file ./models/OssetBERTo-small6/tokenizer.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small6/added_tokens.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small6/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small6/tokenizer_config.json. We won't load it.\n",
      "loading file ./models/OssetBERTo-small6/vocab.json\n",
      "loading file ./models/OssetBERTo-small6/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ./models/OssetBERTo-small6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small6\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n",
      "loading configuration file ./models/OssetBERTo-small6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small6\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    framework = \"pt\",\n",
    "    task = \"fill-mask\",\n",
    "    model=\"./models/OssetBERTo-small6\",\n",
    "    tokenizer=\"./models/OssetBERTo-small6\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0fb4c763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2536475956439972,\n",
       "  'token': 419,\n",
       "  'token_str': ' –º–∞',\n",
       "  'sequence': '—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É, –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É, —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶ –º–∞ –±–∞—Ö–∏–∑.'},\n",
       " {'score': 0.13112694025039673,\n",
       "  'token': 350,\n",
       "  'token_str': ' –Ω√¶',\n",
       "  'sequence': '—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É, –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É, —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶ –Ω√¶ –±–∞—Ö–∏–∑.'},\n",
       " {'score': 0.1002819761633873,\n",
       "  'token': 225,\n",
       "  'token_str': ' ',\n",
       "  'sequence': '—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É, –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É, —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶  –±–∞—Ö–∏–∑.'},\n",
       " {'score': 0.09943877160549164,\n",
       "  'token': 342,\n",
       "  'token_str': ' –¥√¶—Ä',\n",
       "  'sequence': '—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É, –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É, —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶ –¥√¶—Ä –±–∞—Ö–∏–∑.'},\n",
       " {'score': 0.026535294950008392,\n",
       "  'token': 489,\n",
       "  'token_str': ' –Ω–∞–ª',\n",
       "  'sequence': '—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É, –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É, —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶ –Ω–∞–ª –±–∞—Ö–∏–∑.'},\n",
       " {'score': 0.026464218273758888,\n",
       "  'token': 368,\n",
       "  'token_str': ' –∫—É—ã',\n",
       "  'sequence': '—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É, –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É, —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶ –∫—É—ã –±–∞—Ö–∏–∑.'},\n",
       " {'score': 0.025691399350762367,\n",
       "  'token': 2431,\n",
       "  'token_str': ' –º–∞—É–∞–ª',\n",
       "  'sequence': '—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É, –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É, —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶ –º–∞—É–∞–ª –±–∞—Ö–∏–∑.'},\n",
       " {'score': 0.012931283563375473,\n",
       "  'token': 611,\n",
       "  'token_str': ' —É√¶–¥–¥√¶—Ä',\n",
       "  'sequence': '—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É, –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É, —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶ —É√¶–¥–¥√¶—Ä –±–∞—Ö–∏–∑.'},\n",
       " {'score': 0.010351565666496754,\n",
       "  'token': 576,\n",
       "  'token_str': ' –Ω–µ',\n",
       "  'sequence': '—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É, –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É, —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶ –Ω–µ –±–∞—Ö–∏–∑.'},\n",
       " {'score': 0.009472320787608624,\n",
       "  'token': 718,\n",
       "  'token_str': ' —Ö—ä—É–∞–º√¶',\n",
       "  'sequence': '—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É, –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É, —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶ —Ö—ä—É–∞–º√¶ –±–∞—Ö–∏–∑.'}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sun <mask>.\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "fill_mask (\"—á—ã–∑–≥ —É√¶–∑–∑–∞—É —Ä—ã–Ω—á—ã–Ω —É , –¥√¶—É–º√¶ —Ä–∞—Ü√¶—É—ã–Ω√¶–Ω –Ω√¶—É , —Ñ√¶–ª√¶ –π√¶–º –º–∏–¥√¶–º√¶ <mask> –±–∞—Ö–∏–∑ .\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0, mask_token_index][:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb128a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"—Ñ√¶–Ω–¥—ã–¥–∏—Å\"== \"—Ñ√¶–Ω–¥—ã–¥–∏—Å\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27776857",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a08978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36279fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa292848",
   "metadata": {},
   "outputs": [],
   "source": [
    "str([1,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(decoder.decode('ÔøΩ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd04194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class FileDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            self.samples = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fac70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10c9021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/sveta/anaconda3/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "/home/sveta/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1587\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='199' max='199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [199/199 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbessveta\u001b[0m (\u001b[33mbessveta_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/sveta/hdisk4/sveta/translate/osetian/data for mlm/wandb/run-20220911_111036-3j26z9q9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bessveta_team/huggingface/runs/3j26z9q9\" target=\"_blank\">./models/OssetBERTo-small6</a></strong> to <a href=\"https://wandb.ai/bessveta_team/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 125.83\n"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import RobertaConfig\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "   \n",
    "path = \"evaluate.txt\"\n",
    "model_path = \"./models/OssetBERTo-small6\"\n",
    "# Customize training\n",
    "\n",
    "vocab_size = 70_000\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path, max_len=512)\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=path,\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"./models/OssetBERTo-small6\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    eval_dataset=dataset,\n",
    ")\n",
    "eval_results = trainer.evaluate()\n",
    "import math\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d7ab6",
   "metadata": {},
   "source": [
    "# Test phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9227279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "data = pd.read_csv(\"test_phrases.csv\", sep ='\\t')\n",
    "\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    framework = \"pt\",\n",
    "    task = \"fill-mask\",\n",
    "    model=\"./models/OssetBERTo-small6\",\n",
    "    tokenizer=\"./models/OssetBERTo-small6\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1e9973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>masked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>—Å–∞–±–∏—Ç√¶ –¥√¶—Ä √¶–º√¶ —Ö–∏—Å—Ç√¶—Ä—Ç√¶ &lt;mask&gt; –¥–∑—ã—Ä–¥—Ç–æ–π : ‚Äî –Ω–æ...</td>\n",
       "      <td>masked:–¥√¶—Ä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;mask&gt; —Ç—ã–Ω–≥ –¥—ã–º–≥√¶ –∫√¶–π –∫–æ–¥—Ç–∞ , —É—ã–º√¶ –≥√¶—Å–≥√¶ –π—ã–Ω —Ä...</td>\n",
       "      <td>masked:—Ñ√¶–ª√¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ª–µ–Ω—á—ã—Ç√¶ √¶–º√¶ —Å√¶—Ä–±—ã–Ω—Ç√¶ –∫–æ–¥—Ç–∞ , —Ñ√¶–ª√¶ –π—ã–ª —É√¶–¥–¥√¶—Ä —Ñ...</td>\n",
       "      <td>masked:—Ö—É–¥–≥√¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>√¶–≤√¶—Ü—Ü√¶–≥√¶–Ω , –∞—Ö√¶–º —Ñ—ã–¥–∫–æ–Ω–¥—ã –º—ã–≥–≥–∞–≥ –¥√¶–Ω , √¶–º√¶ –º—ã–ª...</td>\n",
       "      <td>masked:–∫√¶–Ω—ã</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>—Ü—ã–±—ã—Ä –¥–∑—ã—Ä–¥√¶–π , –¥–∑√¶–Ω√¶—Ç √¶–º√¶ &lt;mask&gt; !</td>\n",
       "      <td>masked:–¥–∑√¶–Ω√¶—Ç</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          masked\n",
       "0  —Å–∞–±–∏—Ç√¶ –¥√¶—Ä √¶–º√¶ —Ö–∏—Å—Ç√¶—Ä—Ç√¶ <mask> –¥–∑—ã—Ä–¥—Ç–æ–π : ‚Äî –Ω–æ...      masked:–¥√¶—Ä\n",
       "1  <mask> —Ç—ã–Ω–≥ –¥—ã–º–≥√¶ –∫√¶–π –∫–æ–¥—Ç–∞ , —É—ã–º√¶ –≥√¶—Å–≥√¶ –π—ã–Ω —Ä...     masked:—Ñ√¶–ª√¶\n",
       "2  –ª–µ–Ω—á—ã—Ç√¶ √¶–º√¶ —Å√¶—Ä–±—ã–Ω—Ç√¶ –∫–æ–¥—Ç–∞ , —Ñ√¶–ª√¶ –π—ã–ª —É√¶–¥–¥√¶—Ä —Ñ...    masked:—Ö—É–¥–≥√¶\n",
       "3  √¶–≤√¶—Ü—Ü√¶–≥√¶–Ω , –∞—Ö√¶–º —Ñ—ã–¥–∫–æ–Ω–¥—ã –º—ã–≥–≥–∞–≥ –¥√¶–Ω , √¶–º√¶ –º—ã–ª...     masked:–∫√¶–Ω—ã\n",
       "4                —Ü—ã–±—ã—Ä –¥–∑—ã—Ä–¥√¶–π , –¥–∑√¶–Ω√¶—Ç √¶–º√¶ <mask> !   masked:–¥–∑√¶–Ω√¶—Ç"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb048757",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame()\n",
    "for i, row in data.iterrows():\n",
    "    new_row = {}\n",
    "    new_row['text'] = row['text'].lower()\n",
    "    #print(new_row['text'])\n",
    "    new_row['maked word'] = row['masked']\n",
    "    results = fill_mask(new_row['text'],top_k=10)\n",
    "    words = []\n",
    "    scores = []\n",
    "    for res in results:\n",
    "        words.append(res['token_str'])\n",
    "        scores.append(f\"{res['score']}.2f\") \n",
    "    new_row['predicted_words'] = \", \".join(words)\n",
    "    new_row['scores']  = \", \".join(scores)\n",
    "    new_data = new_data.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b0a5a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>maked word</th>\n",
       "      <th>predicted_words</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>—Å–∞–±–∏—Ç√¶ –¥√¶—Ä √¶–º√¶ —Ö–∏—Å—Ç√¶—Ä—Ç√¶ &lt;mask&gt; –¥–∑—ã—Ä–¥—Ç–æ–π : ‚Äî –Ω–æ...</td>\n",
       "      <td>masked:–¥√¶—Ä</td>\n",
       "      <td>–¥√¶—Ä,  –∞—Ñ—Ç√¶,  ,  –∏—É—É—ã–ª–¥√¶—Ä,  —Ç–∞,  –∞—Ä√¶—Ö,  —Å—ã–Ω,  ...</td>\n",
       "      <td>0.25424066185951233.2f, 0.17507866024971008.2f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;mask&gt; —Ç—ã–Ω–≥ –¥—ã–º–≥√¶ –∫√¶–π –∫–æ–¥—Ç–∞ , —É—ã–º√¶ –≥√¶—Å–≥√¶ –π—ã–Ω —Ä...</td>\n",
       "      <td>masked:—Ñ√¶–ª√¶</td>\n",
       "      <td>—É—ã–π, —Ñ√¶–ª√¶, –π√¶—Ö–∏, —á—ã–∑–≥, √¶–º√¶, –∞—Ñ—Ç√¶, –ª√¶–ø–ø—É, –ª√¶–≥, ...</td>\n",
       "      <td>0.04379870370030403.2f, 0.02910236455500126.2f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ª–µ–Ω—á—ã—Ç√¶ √¶–º√¶ —Å√¶—Ä–±—ã–Ω—Ç√¶ –∫–æ–¥—Ç–∞ , —Ñ√¶–ª√¶ –π—ã–ª —É√¶–¥–¥√¶—Ä —Ñ...</td>\n",
       "      <td>masked:—Ö—É–¥–≥√¶</td>\n",
       "      <td>√¶–º√¶, ,,  –∞–¥√¶–º,  –∞—Ñ—Ç√¶,  –∏—É—É—ã–ª–¥√¶—Ä,  –∑√¶—Ö—Ö—ã–ª,  –±–∏...</td>\n",
       "      <td>0.4476054608821869.2f, 0.04779574275016785.2f,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>√¶–≤√¶—Ü—Ü√¶–≥√¶–Ω , –∞—Ö√¶–º —Ñ—ã–¥–∫–æ–Ω–¥—ã –º—ã–≥–≥–∞–≥ –¥√¶–Ω , √¶–º√¶ –º—ã–ª...</td>\n",
       "      <td>masked:–∫√¶–Ω—ã</td>\n",
       "      <td>–∫–æ–¥—Ç–æ–Ω,  –∫–æ–¥—Ç–∞,  –∫√¶–Ω,  –∫√¶–Ω—ã,  –∫√¶–Ω–¥–∑—ã–Ω√¶–Ω, –¥–∑—ã–Ω...</td>\n",
       "      <td>0.08316395431756973.2f, 0.07217926532030106.2f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>—Ü—ã–±—ã—Ä –¥–∑—ã—Ä–¥√¶–π , –¥–∑√¶–Ω√¶—Ç √¶–º√¶ &lt;mask&gt; !</td>\n",
       "      <td>masked:–¥–∑√¶–Ω√¶—Ç</td>\n",
       "      <td>—Ö–æ—Ä–∑,  –±—É–∑–Ω—ã–≥,  –∫–∞–¥,  —Ü–∞—Ä–¥,  —Å—ã–≥—ä–¥√¶–≥,  —Ñ—ã–¥,  ...</td>\n",
       "      <td>0.011761492118239403.2f, 0.005746391136199236....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>—Ü—ã –º–∞ —É–∞ &lt;mask&gt; —Ä√¶—Å—É–≥—ä–¥–¥√¶—Ä !</td>\n",
       "      <td>masked:—É—ã–º√¶–π</td>\n",
       "      <td>,,  ,  —É—ã–º√¶–π,  –∞—Ö√¶–º,  —É—ã–π,  —Ç—ã–Ω–≥,  —É—ã—Ü—ã,  ‚Äî,  ...</td>\n",
       "      <td>0.15645894408226013.2f, 0.09610989689826965.2f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>–∏—É –π√¶ —Å√¶—Ä—ã–ª –∫—É—ã—Ä–æ–π—ã —Ü–∞–ª—Ö –Ω√¶ —Ä–∞–∑—ã–ª–¥ , √¶–Ω–¥√¶—Ä —Ü—ã ...</td>\n",
       "      <td>masked:–±–∏—Ä√¶</td>\n",
       "      <td>–¥–∑—ã,  –¥√¶—Ä,  —ã–Ω,  ,  —Ä√¶—Å—Ç√¶–≥,  —Ä–∞–Ω,  –±—ã–Ω–∞—Ç,  –∫√¶...</td>\n",
       "      <td>0.07492771744728088.2f, 0.029205668717622757.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>–±–∞–±—ã–∑—ã –ª√¶–ø–ø—ã–Ω —Ç–∞ —Ñ√¶—Å—Ç√¶–º√¶ –π√¶ —Ü—ä—ã–º–∞—Ä–∞–º√¶ –±–∞—Ñ—Ç—ã–¥–∏ ...</td>\n",
       "      <td>masked:–∞—Å—Ç√¶—É–º√¶</td>\n",
       "      <td>—Ö—É—ã–∑√¶–Ω,  —Ü√¶—Å—Ç—ã—Ç√¶,  —Ç–∞,  –¥√¶—Ä,  —Å—Ç—ã,  –±—ã–Ω,  –∫–æ–¥...</td>\n",
       "      <td>0.016999268904328346.2f, 0.008138603530824184....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>&lt;mask&gt; —Ä–∞–≥√¶–π–≥–æ–º–∞—É —Å—ã–ª –±–∞–¥—Ç–∏ √¶–º√¶ , —Ä–∞—Å—Ç –∑√¶–≥—ä—ã–Ω ...</td>\n",
       "      <td>masked:√¶–≥√¶—Ä</td>\n",
       "      <td>\", –∏—É, —É—ã–π, —Å√¶, —Ü—ã–¥√¶—Ä, —Ç—ã–Ω–≥, –∞—Ñ—Ç√¶, —Ñ√¶–ª√¶, –π√¶, -</td>\n",
       "      <td>0.12341205775737762.2f, 0.024369344115257263.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>–¥–µ–Ω–¥–∂—ã–∑—ã—Ö—ä–∞–∑—Ç√¶ –π√¶ –∫—É—ã –∞—É—ã–¥—Ç–æ–π , —É√¶–¥ —Å√¶ –±–∞–∑—ã—Ä—Ç√¶...</td>\n",
       "      <td>masked:—É—ã–¥–æ–Ω</td>\n",
       "      <td>—É—ã–¥–æ–Ω,  —É—ã–π,  –∏–Ω–Ω√¶—Ç√¶,  –Ω—ã—Ä,  ,  —Å√¶—Ö√¶–¥√¶–≥,  –¥—ã—É...</td>\n",
       "      <td>0.286251962184906.2f, 0.06628645211458206.2f, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text       maked word  \\\n",
       "0    —Å–∞–±–∏—Ç√¶ –¥√¶—Ä √¶–º√¶ —Ö–∏—Å—Ç√¶—Ä—Ç√¶ <mask> –¥–∑—ã—Ä–¥—Ç–æ–π : ‚Äî –Ω–æ...       masked:–¥√¶—Ä   \n",
       "1    <mask> —Ç—ã–Ω–≥ –¥—ã–º–≥√¶ –∫√¶–π –∫–æ–¥—Ç–∞ , —É—ã–º√¶ –≥√¶—Å–≥√¶ –π—ã–Ω —Ä...      masked:—Ñ√¶–ª√¶   \n",
       "2    –ª–µ–Ω—á—ã—Ç√¶ √¶–º√¶ —Å√¶—Ä–±—ã–Ω—Ç√¶ –∫–æ–¥—Ç–∞ , —Ñ√¶–ª√¶ –π—ã–ª —É√¶–¥–¥√¶—Ä —Ñ...     masked:—Ö—É–¥–≥√¶   \n",
       "3    √¶–≤√¶—Ü—Ü√¶–≥√¶–Ω , –∞—Ö√¶–º —Ñ—ã–¥–∫–æ–Ω–¥—ã –º—ã–≥–≥–∞–≥ –¥√¶–Ω , √¶–º√¶ –º—ã–ª...      masked:–∫√¶–Ω—ã   \n",
       "4                  —Ü—ã–±—ã—Ä –¥–∑—ã—Ä–¥√¶–π , –¥–∑√¶–Ω√¶—Ç √¶–º√¶ <mask> !    masked:–¥–∑√¶–Ω√¶—Ç   \n",
       "..                                                 ...              ...   \n",
       "171                       —Ü—ã –º–∞ —É–∞ <mask> —Ä√¶—Å—É–≥—ä–¥–¥√¶—Ä !     masked:—É—ã–º√¶–π   \n",
       "172  –∏—É –π√¶ —Å√¶—Ä—ã–ª –∫—É—ã—Ä–æ–π—ã —Ü–∞–ª—Ö –Ω√¶ —Ä–∞–∑—ã–ª–¥ , √¶–Ω–¥√¶—Ä —Ü—ã ...      masked:–±–∏—Ä√¶   \n",
       "173  –±–∞–±—ã–∑—ã –ª√¶–ø–ø—ã–Ω —Ç–∞ —Ñ√¶—Å—Ç√¶–º√¶ –π√¶ —Ü—ä—ã–º–∞—Ä–∞–º√¶ –±–∞—Ñ—Ç—ã–¥–∏ ...   masked:–∞—Å—Ç√¶—É–º√¶   \n",
       "174  <mask> —Ä–∞–≥√¶–π–≥–æ–º–∞—É —Å—ã–ª –±–∞–¥—Ç–∏ √¶–º√¶ , —Ä–∞—Å—Ç –∑√¶–≥—ä—ã–Ω ...      masked:√¶–≥√¶—Ä   \n",
       "175  –¥–µ–Ω–¥–∂—ã–∑—ã—Ö—ä–∞–∑—Ç√¶ –π√¶ –∫—É—ã –∞—É—ã–¥—Ç–æ–π , —É√¶–¥ —Å√¶ –±–∞–∑—ã—Ä—Ç√¶...     masked:—É—ã–¥–æ–Ω   \n",
       "\n",
       "                                       predicted_words  \\\n",
       "0     –¥√¶—Ä,  –∞—Ñ—Ç√¶,  ,  –∏—É—É—ã–ª–¥√¶—Ä,  —Ç–∞,  –∞—Ä√¶—Ö,  —Å—ã–Ω,  ...   \n",
       "1    —É—ã–π, —Ñ√¶–ª√¶, –π√¶—Ö–∏, —á—ã–∑–≥, √¶–º√¶, –∞—Ñ—Ç√¶, –ª√¶–ø–ø—É, –ª√¶–≥, ...   \n",
       "2     √¶–º√¶, ,,  –∞–¥√¶–º,  –∞—Ñ—Ç√¶,  –∏—É—É—ã–ª–¥√¶—Ä,  –∑√¶—Ö—Ö—ã–ª,  –±–∏...   \n",
       "3     –∫–æ–¥—Ç–æ–Ω,  –∫–æ–¥—Ç–∞,  –∫√¶–Ω,  –∫√¶–Ω—ã,  –∫√¶–Ω–¥–∑—ã–Ω√¶–Ω, –¥–∑—ã–Ω...   \n",
       "4     —Ö–æ—Ä–∑,  –±—É–∑–Ω—ã–≥,  –∫–∞–¥,  —Ü–∞—Ä–¥,  —Å—ã–≥—ä–¥√¶–≥,  —Ñ—ã–¥,  ...   \n",
       "..                                                 ...   \n",
       "171  ,,  ,  —É—ã–º√¶–π,  –∞—Ö√¶–º,  —É—ã–π,  —Ç—ã–Ω–≥,  —É—ã—Ü—ã,  ‚Äî,  ...   \n",
       "172   –¥–∑—ã,  –¥√¶—Ä,  —ã–Ω,  ,  —Ä√¶—Å—Ç√¶–≥,  —Ä–∞–Ω,  –±—ã–Ω–∞—Ç,  –∫√¶...   \n",
       "173   —Ö—É—ã–∑√¶–Ω,  —Ü√¶—Å—Ç—ã—Ç√¶,  —Ç–∞,  –¥√¶—Ä,  —Å—Ç—ã,  –±—ã–Ω,  –∫–æ–¥...   \n",
       "174     \", –∏—É, —É—ã–π, —Å√¶, —Ü—ã–¥√¶—Ä, —Ç—ã–Ω–≥, –∞—Ñ—Ç√¶, —Ñ√¶–ª√¶, –π√¶, -   \n",
       "175   —É—ã–¥–æ–Ω,  —É—ã–π,  –∏–Ω–Ω√¶—Ç√¶,  –Ω—ã—Ä,  ,  —Å√¶—Ö√¶–¥√¶–≥,  –¥—ã—É...   \n",
       "\n",
       "                                                scores  \n",
       "0    0.25424066185951233.2f, 0.17507866024971008.2f...  \n",
       "1    0.04379870370030403.2f, 0.02910236455500126.2f...  \n",
       "2    0.4476054608821869.2f, 0.04779574275016785.2f,...  \n",
       "3    0.08316395431756973.2f, 0.07217926532030106.2f...  \n",
       "4    0.011761492118239403.2f, 0.005746391136199236....  \n",
       "..                                                 ...  \n",
       "171  0.15645894408226013.2f, 0.09610989689826965.2f...  \n",
       "172  0.07492771744728088.2f, 0.029205668717622757.2...  \n",
       "173  0.016999268904328346.2f, 0.008138603530824184....  \n",
       "174  0.12341205775737762.2f, 0.024369344115257263.2...  \n",
       "175  0.286251962184906.2f, 0.06628645211458206.2f, ...  \n",
       "\n",
       "[176 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9de3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv(\"test_results_utenok.csv\", index = False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4826ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glgjj\n"
     ]
    }
   ],
   "source": [
    "i = 2900\n",
    "if i% 1000 == 0:\n",
    "    print(\"glgjj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe4bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa80d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.Dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
