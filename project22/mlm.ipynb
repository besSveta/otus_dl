{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b18fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers import  normalizers, decoders\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from tokenizers.processors import RobertaProcessing\n",
    "path = \"osetian_corpus6.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67f16a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/OssetBERTo-small7/vocab.json',\n",
       " './models/OssetBERTo-small7/merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize a tokenizer\n",
    "'''\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"\\n\", \"▁\"),# this is *NOT* underscore but a rare unicode char\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents()\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "#bert_normalizer = BertNormalizer(clean_text=True,handle_chinese_chars=False)\n",
    "#tokenizer._tokenizer.normalizer =  normalizers.Sequence([bert_normalizer, normalizers.NFKC()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "vocab_size = 60_000\n",
    "max_length = 512\n",
    "# Customize training\n",
    "tokenizer.train(files=path, vocab_size=vocab_size, min_frequency=2, special_tokens=[\n",
    "   \"[PAD]\",\"[UNK]\",\"[MASK]\", \"[SEP]\",\"[CLS]\"\n",
    "])\n",
    "\n",
    "tokenizer._tokenizer.post_processor = RobertaProcessing((\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")), \n",
    "                                                         (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),)\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_model(\"./models/OssetBERTo-small7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files to disk\n",
    "#tokenizer.save_model(\"./models/OssetBERTo-small3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa983675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./models/OssetBERTo-small6\"\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path, )\n",
    "tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a456b38",
   "metadata": {},
   "source": [
    "import json\n",
    "with open(\"osset_bert-vocab.json\", encoding=\"utf-8\") as p:\n",
    "    jsonstr =  json.loads(p.read())\n",
    "jsonstr    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20cc068",
   "metadata": {},
   "source": [
    "with open(\"osset_bert-merges.txt\", encoding=\"utf-8\", mode=\"r\") as p:\n",
    "    merges =  p.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7bf84e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=3, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'ÑĥÐ°ÑĨÐ°Ñĥ', '[SEP]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from tokenizers import implementations , Tokenizer\n",
    "from tokenizers.processors import RobertaProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer.from_file(\"./models/OssetBERTo-small7/vocab.json\", \n",
    "                                            \"./models/OssetBERTo-small7/merges.txt\")\n",
    "tokenizer._tokenizer.post_processor = RobertaProcessing(\n",
    "(\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")), (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "print(\n",
    "    tokenizer.encode(\"уацау\")\n",
    ")\n",
    "# Encoding(num_tokens=7, ...)\n",
    "# tokens: ['<s>', 'Mi', 'Ġestas', 'ĠJuli', 'en', '.', '</s>']\n",
    "m = tokenizer.encode(\"уацау\")\n",
    "m.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.id_to_token(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e66566c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id('<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a889418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "уацау\n",
      "уацау\n",
      "уацау\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.decoders import ByteLevel\n",
    "decoder = ByteLevel()\n",
    "for token in m.tokens:\n",
    "    print(decoder.decode( 'ÑĥÐ°ÑĨÐ°Ñĥ'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ad60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loader\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from datasets import load_dataset\n",
    "model_path = \"./models/OssetBERTo-small4\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path, max_len=512)\n",
    "file_path  = f\"osetian_corpus4.txt\"\n",
    "dataset = load_dataset('text', data_files={'train':file_path})\n",
    "tokenized_data = dataset.map( preprocess_function, batched=True, num_proc=4, \n",
    "                             remove_columns=dataset[\"train\"].column_names, )\n",
    "block_size = 128\n",
    "dataset =  tokenized_data.map(group_texts, batched=True, batch_size=1000, num_proc=4)\n",
    "\n",
    "dataset.column_names\n",
    "\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, )\n",
    "dataset.set_format(\"torch\")\n",
    "loader = torch.utils.data.DataLoader(dataset['train'], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd9bb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60000, 57240, 60001]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('уацау')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45a89f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>уацау</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([60000, 57240, 60001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c6005",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39515af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import  normalizers, decoders\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaConfig\n",
    "vocab_size = 100_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f278e16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e569848209fdac70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/sveta/.cache/huggingface/datasets/text/default-e569848209fdac70/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54b5ca7c2314855ab568693e7cba9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20281bb5e1204df5a6321efc4d4440ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/sveta/.cache/huggingface/datasets/text/default-e569848209fdac70/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4452100a9b41498095f799ad0ce21255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./models/OssetBERTo-small5/tokenizer.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small5/added_tokens.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small5/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small5/tokenizer_config.json. We won't load it.\n",
      "loading file ./models/OssetBERTo-small5/vocab.json\n",
      "loading file ./models/OssetBERTo-small5/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ./models/OssetBERTo-small5/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small5\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n",
      "loading configuration file ./models/OssetBERTo-small5/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small5\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "file_path  = f\"osetian_corpus5.txt\"\n",
    "dataset = load_dataset('text', data_files={'train':file_path})\n",
    "dataset  = dataset['train']\n",
    "dataset  = dataset.train_test_split(test_size=0.15)\n",
    "\n",
    "\n",
    "model_path = \"./models/OssetBERTo-small5\"\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94155c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer( examples['text'])\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a92604",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "#dataset = dataset.flatten()\n",
    "#batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n",
    "\n",
    "tokenized_data = dataset.map( preprocess_function, batched=True, num_proc=4, \n",
    "                             remove_columns=dataset[\"train\"].column_names, )\n",
    "dataset =  tokenized_data.map(group_texts, batched=True, batch_size=1000, num_proc=4)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size= vocab_size,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_gpu_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset= dataset[\"test\"],\n",
    ")\n",
    "old_collator = trainer.data_collator\n",
    "trainer.data_collator = lambda data: dict(old_collator(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_path)\n",
    "#import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfc25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "import math\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3a4b6",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a8ba7872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models/OssetBERTo-small6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small6\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n",
      "loading configuration file ./models/OssetBERTo-small6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small6\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n",
      "loading weights file ./models/OssetBERTo-small6/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./models/OssetBERTo-small6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ./models/OssetBERTo-small6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small6\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n",
      "Didn't find file ./models/OssetBERTo-small6/tokenizer.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small6/added_tokens.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small6/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./models/OssetBERTo-small6/tokenizer_config.json. We won't load it.\n",
      "loading file ./models/OssetBERTo-small6/vocab.json\n",
      "loading file ./models/OssetBERTo-small6/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ./models/OssetBERTo-small6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small6\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n",
      "loading configuration file ./models/OssetBERTo-small6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/OssetBERTo-small6\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    framework = \"pt\",\n",
    "    task = \"fill-mask\",\n",
    "    model=\"./models/OssetBERTo-small6\",\n",
    "    tokenizer=\"./models/OssetBERTo-small6\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0fb4c763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2536475956439972,\n",
       "  'token': 419,\n",
       "  'token_str': ' ма',\n",
       "  'sequence': 'чызг уæззау рынчын у, дæумæ рацæуынæн нæу, фæлæ йæм мидæмæ ма бахиз.'},\n",
       " {'score': 0.13112694025039673,\n",
       "  'token': 350,\n",
       "  'token_str': ' нæ',\n",
       "  'sequence': 'чызг уæззау рынчын у, дæумæ рацæуынæн нæу, фæлæ йæм мидæмæ нæ бахиз.'},\n",
       " {'score': 0.1002819761633873,\n",
       "  'token': 225,\n",
       "  'token_str': ' ',\n",
       "  'sequence': 'чызг уæззау рынчын у, дæумæ рацæуынæн нæу, фæлæ йæм мидæмæ  бахиз.'},\n",
       " {'score': 0.09943877160549164,\n",
       "  'token': 342,\n",
       "  'token_str': ' дæр',\n",
       "  'sequence': 'чызг уæззау рынчын у, дæумæ рацæуынæн нæу, фæлæ йæм мидæмæ дæр бахиз.'},\n",
       " {'score': 0.026535294950008392,\n",
       "  'token': 489,\n",
       "  'token_str': ' нал',\n",
       "  'sequence': 'чызг уæззау рынчын у, дæумæ рацæуынæн нæу, фæлæ йæм мидæмæ нал бахиз.'},\n",
       " {'score': 0.026464218273758888,\n",
       "  'token': 368,\n",
       "  'token_str': ' куы',\n",
       "  'sequence': 'чызг уæззау рынчын у, дæумæ рацæуынæн нæу, фæлæ йæм мидæмæ куы бахиз.'},\n",
       " {'score': 0.025691399350762367,\n",
       "  'token': 2431,\n",
       "  'token_str': ' мауал',\n",
       "  'sequence': 'чызг уæззау рынчын у, дæумæ рацæуынæн нæу, фæлæ йæм мидæмæ мауал бахиз.'},\n",
       " {'score': 0.012931283563375473,\n",
       "  'token': 611,\n",
       "  'token_str': ' уæддæр',\n",
       "  'sequence': 'чызг уæззау рынчын у, дæумæ рацæуынæн нæу, фæлæ йæм мидæмæ уæддæр бахиз.'},\n",
       " {'score': 0.010351565666496754,\n",
       "  'token': 576,\n",
       "  'token_str': ' не',\n",
       "  'sequence': 'чызг уæззау рынчын у, дæумæ рацæуынæн нæу, фæлæ йæм мидæмæ не бахиз.'},\n",
       " {'score': 0.009472320787608624,\n",
       "  'token': 718,\n",
       "  'token_str': ' хъуамæ',\n",
       "  'sequence': 'чызг уæззау рынчын у, дæумæ рацæуынæн нæу, фæлæ йæм мидæмæ хъуамæ бахиз.'}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sun <mask>.\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "fill_mask (\"чызг уæззау рынчын у , дæумæ рацæуынæн нæу , фæлæ йæм мидæмæ <mask> бахиз .\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0, mask_token_index][:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb128a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"фæндыдис\"== \"фæндыдис\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27776857",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a08978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36279fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa292848",
   "metadata": {},
   "outputs": [],
   "source": [
    "str([1,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(decoder.decode('�'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd04194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class FileDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            self.samples = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fac70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10c9021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/sveta/anaconda3/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "/home/sveta/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1587\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='199' max='199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [199/199 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbessveta\u001b[0m (\u001b[33mbessveta_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/sveta/hdisk4/sveta/translate/osetian/data for mlm/wandb/run-20220911_111036-3j26z9q9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bessveta_team/huggingface/runs/3j26z9q9\" target=\"_blank\">./models/OssetBERTo-small6</a></strong> to <a href=\"https://wandb.ai/bessveta_team/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 125.83\n"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import RobertaConfig\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "   \n",
    "path = \"evaluate.txt\"\n",
    "model_path = \"./models/OssetBERTo-small6\"\n",
    "# Customize training\n",
    "\n",
    "vocab_size = 70_000\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path, max_len=512)\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=path,\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"./models/OssetBERTo-small6\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    eval_dataset=dataset,\n",
    ")\n",
    "eval_results = trainer.evaluate()\n",
    "import math\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d7ab6",
   "metadata": {},
   "source": [
    "# Test phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9227279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "data = pd.read_csv(\"test_phrases.csv\", sep ='\\t')\n",
    "\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    framework = \"pt\",\n",
    "    task = \"fill-mask\",\n",
    "    model=\"./models/OssetBERTo-small6\",\n",
    "    tokenizer=\"./models/OssetBERTo-small6\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1e9973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>masked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>сабитæ дæр æмæ хистæртæ &lt;mask&gt; дзырдтой : — но...</td>\n",
       "      <td>masked:дæр</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;mask&gt; тынг дымгæ кæй кодта , уымæ гæсгæ йын р...</td>\n",
       "      <td>masked:фæлæ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ленчытæ æмæ сæрбынтæ кодта , фæлæ йыл уæддæр ф...</td>\n",
       "      <td>masked:худгæ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>æвæццæгæн , ахæм фыдконды мыггаг дæн , æмæ мыл...</td>\n",
       "      <td>masked:кæны</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>цыбыр дзырдæй , дзæнæт æмæ &lt;mask&gt; !</td>\n",
       "      <td>masked:дзæнæт</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          masked\n",
       "0  сабитæ дæр æмæ хистæртæ <mask> дзырдтой : — но...      masked:дæр\n",
       "1  <mask> тынг дымгæ кæй кодта , уымæ гæсгæ йын р...     masked:фæлæ\n",
       "2  ленчытæ æмæ сæрбынтæ кодта , фæлæ йыл уæддæр ф...    masked:худгæ\n",
       "3  æвæццæгæн , ахæм фыдконды мыггаг дæн , æмæ мыл...     masked:кæны\n",
       "4                цыбыр дзырдæй , дзæнæт æмæ <mask> !   masked:дзæнæт"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb048757",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame()\n",
    "for i, row in data.iterrows():\n",
    "    new_row = {}\n",
    "    new_row['text'] = row['text'].lower()\n",
    "    #print(new_row['text'])\n",
    "    new_row['maked word'] = row['masked']\n",
    "    results = fill_mask(new_row['text'],top_k=10)\n",
    "    words = []\n",
    "    scores = []\n",
    "    for res in results:\n",
    "        words.append(res['token_str'])\n",
    "        scores.append(f\"{res['score']}.2f\") \n",
    "    new_row['predicted_words'] = \", \".join(words)\n",
    "    new_row['scores']  = \", \".join(scores)\n",
    "    new_data = new_data.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b0a5a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>maked word</th>\n",
       "      <th>predicted_words</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>сабитæ дæр æмæ хистæртæ &lt;mask&gt; дзырдтой : — но...</td>\n",
       "      <td>masked:дæр</td>\n",
       "      <td>дæр,  афтæ,  ,  иууылдæр,  та,  арæх,  сын,  ...</td>\n",
       "      <td>0.25424066185951233.2f, 0.17507866024971008.2f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;mask&gt; тынг дымгæ кæй кодта , уымæ гæсгæ йын р...</td>\n",
       "      <td>masked:фæлæ</td>\n",
       "      <td>уый, фæлæ, йæхи, чызг, æмæ, афтæ, лæппу, лæг, ...</td>\n",
       "      <td>0.04379870370030403.2f, 0.02910236455500126.2f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ленчытæ æмæ сæрбынтæ кодта , фæлæ йыл уæддæр ф...</td>\n",
       "      <td>masked:худгæ</td>\n",
       "      <td>æмæ, ,,  адæм,  афтæ,  иууылдæр,  зæххыл,  би...</td>\n",
       "      <td>0.4476054608821869.2f, 0.04779574275016785.2f,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>æвæццæгæн , ахæм фыдконды мыггаг дæн , æмæ мыл...</td>\n",
       "      <td>masked:кæны</td>\n",
       "      <td>кодтон,  кодта,  кæн,  кæны,  кæндзынæн, дзын...</td>\n",
       "      <td>0.08316395431756973.2f, 0.07217926532030106.2f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>цыбыр дзырдæй , дзæнæт æмæ &lt;mask&gt; !</td>\n",
       "      <td>masked:дзæнæт</td>\n",
       "      <td>хорз,  бузныг,  кад,  цард,  сыгъдæг,  фыд,  ...</td>\n",
       "      <td>0.011761492118239403.2f, 0.005746391136199236....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>цы ма уа &lt;mask&gt; рæсугъддæр !</td>\n",
       "      <td>masked:уымæй</td>\n",
       "      <td>,,  ,  уымæй,  ахæм,  уый,  тынг,  уыцы,  —,  ...</td>\n",
       "      <td>0.15645894408226013.2f, 0.09610989689826965.2f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>иу йæ сæрыл куыройы цалх нæ разылд , æндæр цы ...</td>\n",
       "      <td>masked:бирæ</td>\n",
       "      <td>дзы,  дæр,  ын,  ,  рæстæг,  ран,  бынат,  кæ...</td>\n",
       "      <td>0.07492771744728088.2f, 0.029205668717622757.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>бабызы лæппын та фæстæмæ йæ цъымарамæ бафтыди ...</td>\n",
       "      <td>masked:астæумæ</td>\n",
       "      <td>хуызæн,  цæстытæ,  та,  дæр,  сты,  бын,  код...</td>\n",
       "      <td>0.016999268904328346.2f, 0.008138603530824184....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>&lt;mask&gt; рагæйгомау сыл бадти æмæ , раст зæгъын ...</td>\n",
       "      <td>masked:æгæр</td>\n",
       "      <td>\", иу, уый, сæ, цыдæр, тынг, афтæ, фæлæ, йæ, -</td>\n",
       "      <td>0.12341205775737762.2f, 0.024369344115257263.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>денджызыхъазтæ йæ куы ауыдтой , уæд сæ базыртæ...</td>\n",
       "      <td>masked:уыдон</td>\n",
       "      <td>уыдон,  уый,  иннæтæ,  ныр,  ,  сæхæдæг,  дыу...</td>\n",
       "      <td>0.286251962184906.2f, 0.06628645211458206.2f, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text       maked word  \\\n",
       "0    сабитæ дæр æмæ хистæртæ <mask> дзырдтой : — но...       masked:дæр   \n",
       "1    <mask> тынг дымгæ кæй кодта , уымæ гæсгæ йын р...      masked:фæлæ   \n",
       "2    ленчытæ æмæ сæрбынтæ кодта , фæлæ йыл уæддæр ф...     masked:худгæ   \n",
       "3    æвæццæгæн , ахæм фыдконды мыггаг дæн , æмæ мыл...      masked:кæны   \n",
       "4                  цыбыр дзырдæй , дзæнæт æмæ <mask> !    masked:дзæнæт   \n",
       "..                                                 ...              ...   \n",
       "171                       цы ма уа <mask> рæсугъддæр !     masked:уымæй   \n",
       "172  иу йæ сæрыл куыройы цалх нæ разылд , æндæр цы ...      masked:бирæ   \n",
       "173  бабызы лæппын та фæстæмæ йæ цъымарамæ бафтыди ...   masked:астæумæ   \n",
       "174  <mask> рагæйгомау сыл бадти æмæ , раст зæгъын ...      masked:æгæр   \n",
       "175  денджызыхъазтæ йæ куы ауыдтой , уæд сæ базыртæ...     masked:уыдон   \n",
       "\n",
       "                                       predicted_words  \\\n",
       "0     дæр,  афтæ,  ,  иууылдæр,  та,  арæх,  сын,  ...   \n",
       "1    уый, фæлæ, йæхи, чызг, æмæ, афтæ, лæппу, лæг, ...   \n",
       "2     æмæ, ,,  адæм,  афтæ,  иууылдæр,  зæххыл,  би...   \n",
       "3     кодтон,  кодта,  кæн,  кæны,  кæндзынæн, дзын...   \n",
       "4     хорз,  бузныг,  кад,  цард,  сыгъдæг,  фыд,  ...   \n",
       "..                                                 ...   \n",
       "171  ,,  ,  уымæй,  ахæм,  уый,  тынг,  уыцы,  —,  ...   \n",
       "172   дзы,  дæр,  ын,  ,  рæстæг,  ран,  бынат,  кæ...   \n",
       "173   хуызæн,  цæстытæ,  та,  дæр,  сты,  бын,  код...   \n",
       "174     \", иу, уый, сæ, цыдæр, тынг, афтæ, фæлæ, йæ, -   \n",
       "175   уыдон,  уый,  иннæтæ,  ныр,  ,  сæхæдæг,  дыу...   \n",
       "\n",
       "                                                scores  \n",
       "0    0.25424066185951233.2f, 0.17507866024971008.2f...  \n",
       "1    0.04379870370030403.2f, 0.02910236455500126.2f...  \n",
       "2    0.4476054608821869.2f, 0.04779574275016785.2f,...  \n",
       "3    0.08316395431756973.2f, 0.07217926532030106.2f...  \n",
       "4    0.011761492118239403.2f, 0.005746391136199236....  \n",
       "..                                                 ...  \n",
       "171  0.15645894408226013.2f, 0.09610989689826965.2f...  \n",
       "172  0.07492771744728088.2f, 0.029205668717622757.2...  \n",
       "173  0.016999268904328346.2f, 0.008138603530824184....  \n",
       "174  0.12341205775737762.2f, 0.024369344115257263.2...  \n",
       "175  0.286251962184906.2f, 0.06628645211458206.2f, ...  \n",
       "\n",
       "[176 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9de3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv(\"test_results_utenok.csv\", index = False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4826ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glgjj\n"
     ]
    }
   ],
   "source": [
    "i = 2900\n",
    "if i% 1000 == 0:\n",
    "    print(\"glgjj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe4bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa80d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.Dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
